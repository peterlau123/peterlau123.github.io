<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peter Lau Blog</title>
    <description>这里是 @Peter Lau 刘鑫 的个人博客，与你一起发现更大的世界 | 要做一个有 理想与现实兼顾 的程序员</description>
    <link>https://peterlau123.github.io/</link>
    <atom:link href="https://peterlau123.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 13 Dec 2025 21:41:26 +0800</pubDate>
    <lastBuildDate>Sat, 13 Dec 2025 21:41:26 +0800</lastBuildDate>
    <generator>Jekyll v4.3.4</generator>
    
      <item>
        <title>Prefect intro</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;prefect logo&quot; src=&quot;/img/prefect/prefect_logo.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;Prefect&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;最近经常需要使用python脚本处理数据，在目标场景中数据需要经过多个环节处理才能完成。最初，个人使用多个python脚本，每个脚本对应一个环节，最后再使用shell或者python将这些环节串起来。&lt;/p&gt;

&lt;p&gt;后来发现，这些代码可用性较差、不方便维护且需要引入额外的处理比如日志监控和差错重试等。如果去重构也需要再花些时间去梳理，RoI并不高。有无可能从刚开始就按照工作流的形式规范各个环节呢？有的，使用&lt;a href=&quot;https://docs.prefect.io/v3/get-started/index&quot;&gt;Prefect&lt;/a&gt;！&lt;/p&gt;

&lt;h2 id=&quot;prefect&quot;&gt;Prefect&lt;/h2&gt;

&lt;h3 id=&quot;flow与task&quot;&gt;flow与task&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;prefect&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;


&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My Example Task&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
      &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;An example task for a tutorial.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;task_run_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hello-{name}-on-{date:%A}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;


&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flow&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_flow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# creates a run with a name like &quot;hello-marvin-on-Thursday&quot;
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;my_task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;marvin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timezone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;my_flow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;prefect&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flow&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My Flow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My flow with a name and description&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_prints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_flow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hello, I'm a flow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;my_flow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;使用@flow和@task来标记&lt;strong&gt;任务流&lt;/strong&gt;和&lt;strong&gt;具体任务&lt;/strong&gt;,借此可以开发中便捷地标示出主要模块以及主要流程，不需要引入额外的注释做说明。&lt;/p&gt;

&lt;p&gt;flow的配置可参考&lt;a href=&quot;https://docs.prefect.io/v3/develop/write-flows#flow-settings&quot;&gt;flow-settings&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;task的配置可参考&lt;a href=&quot;https://docs.prefect.io/v3/develop/write-tasks&quot;&gt;task-settings&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;flow可根据场景需求进行定向设置，例如flow编排，状态管理和重试等；同样地，task也可以设置并发度、超时和重试等。&lt;/p&gt;

&lt;h3 id=&quot;flow-monitoring&quot;&gt;flow monitoring&lt;/h3&gt;

&lt;p&gt;本地可以搭建一个prefect flow运行监控服务，操作流程明细可参考&lt;a href=&quot;https://docs.prefect.io/v3/manage/server/index&quot;&gt;Prefect&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;启动本地端口4200的监控服务&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;prefect config &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PREFECT_API_URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://127.0.0.1:4200/api&quot;&lt;/span&gt;

prefect server start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;运行本地按照prefect flow编写的脚本，流程监控如下&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;prefect dashboard&quot; src=&quot;/img/prefect/prefect_dashboard_flow_task.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;Prefect dashboard&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;prefect dashboard 可以总览最近运行的flow和task&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;prefect runs&quot; src=&quot;/img/prefect/prefect_runs.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;Prefect runs&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;prefect runs可以查询各种运行状态的flow和task&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;flow details&quot; src=&quot;/img/prefect/prefect_flow_detail.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;Prefect flow&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;prefect flow可以查询flow运行时的日志和行为&lt;/p&gt;
</description>
        <pubDate>Fri, 28 Mar 2025 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2025/03/28/prefect_framework_intro/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2025/03/28/prefect_framework_intro/</guid>
        
        <category>Computer science</category>
        
        <category>Software</category>
        
        <category>Python</category>
        
        
      </item>
    
      <item>
        <title>Llama</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/llama/llama-logo.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;Llama Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;本文主要介绍Llama的模型结构和主要优化特性。&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/llama/llama_arch.png&quot; width=&quot;500&quot; height=&quot;400&quot; alt=&quot;Llama Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Llama结构整体如上，是个典型的decoder-only结构，它包含如下部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Embedding&lt;/li&gt;
  &lt;li&gt;Transformer stack&lt;/li&gt;
  &lt;li&gt;RMS Norm&lt;/li&gt;
  &lt;li&gt;Linear&lt;/li&gt;
  &lt;li&gt;Softmax&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Transformer stack包含多个Transformer block，每个block包含如下部分：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RMS Norm&lt;/li&gt;
  &lt;li&gt;Rotary positional Encoding&lt;/li&gt;
  &lt;li&gt;Grouped Multi-Query Attention&lt;/li&gt;
  &lt;li&gt;Add&lt;/li&gt;
  &lt;li&gt;RMS Norm&lt;/li&gt;
  &lt;li&gt;Feed Forward SwiGLU&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;与其他开源模型关系&lt;/strong&gt;&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/llama/llama_ope_source.png&quot; width=&quot;400&quot; height=&quot;300&quot; alt=&quot;Llama Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Qwen和DeepSeek都借鉴了Llama的结构&lt;/p&gt;

&lt;h2 id=&quot;主要优化特性&quot;&gt;主要优化特性&lt;/h2&gt;

&lt;h3 id=&quot;rope&quot;&gt;RoPE&lt;/h3&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/llama/RoPE_impl.png&quot; width=&quot;500&quot; height=&quot;200&quot; alt=&quot;Llama Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;假设当前输入三个token，记为$x_1,x_2,x_3$，输出embedding维度为$2$&lt;/p&gt;

&lt;p&gt;设$K$和$V$的线性投影矩阵为$W_k \in R^{2\times2}$，$W_q \in R^{2\times2}$，$W_v \in R^{2\times2}$&lt;/p&gt;

\[K = W_k \cdot X\\
Q = W_q \cdot X\\
V = W_v \cdot X\\
X \in R^{2\times3}\\
X=\begin{bmatrix}
x_1; x_2; x_3
\end{bmatrix}\\\]

&lt;p&gt;我们当前的旋转编码矩阵&lt;/p&gt;

&lt;p&gt;对于$x_2$为
\(R_{\theta,2}^2=
\begin{bmatrix}
\cos(\theta_2) &amp;amp; -\sin(\theta_2) \\
\sin(\theta_2) &amp;amp; \cos(\theta_2)
\end{bmatrix}\)&lt;/p&gt;

&lt;p&gt;编码后
\(R_{\theta,2}^2 \cdot W_k \cdot x_2\\
R_{\theta,2}^2 \cdot W_q \cdot x_2\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:red&quot;&gt;注意是先线性投影再编码&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于$x_3$为
\(R_{\theta,3}^2=
\begin{bmatrix}
\cos(\theta_3) &amp;amp; -\sin(\theta_3) \\
\sin(\theta_3) &amp;amp; \cos(\theta_3)
\end{bmatrix}\)&lt;/p&gt;

&lt;p&gt;编码后
\(X_3=R_{\theta,3}^2 \cdot W_k \cdot x_3 \\
X_3=R_{\theta,3}^2 \cdot W_q \cdot x_3\)&lt;/p&gt;

&lt;p&gt;那么当$K$与$Q$做点击积时$Q^TK$，对于$Q$的第二列与$K$的第三行
\((R_{\theta,2}^2W_qx_2)^T(R_{\theta,3}^2W_kx_3)=X_2^T W_q^T {R_{\theta,2}^{2\top} R_{\theta,3}^2} W_kx_3\)&lt;/p&gt;

\[R_{\theta,2}^{2\top} R_{\theta,3}^2 = \begin{pmatrix}
\cos(\theta_2 - \theta_3) &amp;amp; -\sin(\theta_2 - \theta_3) \\
\sin(\theta_2 - \theta_3) &amp;amp; \cos(\theta_2 - \theta_3) \\
\end{pmatrix}\]

&lt;p&gt;如上所示，第2个和第3个token的相对位置信息被RoPE带入了进来&lt;/p&gt;

&lt;p&gt;可以推广到高维情景，如下&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/llama/rope_matrix.png&quot; width=&quot;500&quot; height=&quot;200&quot; alt=&quot;Llama Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

\[q_{m}^{\top} \boldsymbol{k}_{n} 
= \left( R_{\Theta, m}^{d} \boldsymbol{W}_{q} \boldsymbol{x}_{m} \right)^{\! \top} \!\left( R_{\Theta, n}^{d} \boldsymbol{W}_{k} \boldsymbol{x}_{n} \right) 
= \boldsymbol{x}_{m}^{\top} \boldsymbol{W}_{q}^{\top} R_{\Theta, n - m}^{d} \boldsymbol{W}_{k} \boldsymbol{x}_{n}\]

&lt;p&gt;其中&lt;/p&gt;

\[R_{\theta,n-m}^d=(R_{\theta,m}^d)^TR_{\theta,n}^d\]

&lt;h3 id=&quot;grouped-multi-query-attention&quot;&gt;Grouped Multi-Query Attention&lt;/h3&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/llama/Group-Query.png&quot; width=&quot;500&quot; height=&quot;200&quot; alt=&quot;Llama Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;以输入100个token为例，token的embedding维度为512&lt;/p&gt;

&lt;p&gt;注意力头数$h=8$，分组数$num_heads=2$&lt;/p&gt;

&lt;p&gt;输入$X \in R^{100 \times 512}$，$8$个注意力头划分为$2$个组&lt;/p&gt;

\[Q = XW^Q, W^Q \in R^{512\times512}\\
K = XW^K, W^K \in R^{512\times128}\\
V = XW^V, W^V \in R^{512\times128}\]

&lt;p&gt;此时的$Q \in R^{100 \times 512}$，$K \in R^{100 \times 128}$，$V \in R^{100 \times 128}$&lt;/p&gt;

&lt;p&gt;每个注意力头的维度为64，每组k和v的维度为64且被组内4个注意力头共享&lt;/p&gt;

&lt;p&gt;注意力计算公式如下&lt;/p&gt;

\[head_i=softmax(\frac{Q_iK_g^T}{\sqrt{d_k}})V_g\\
Q_i \in R^{100\times64} , 1&amp;lt;=i&amp;lt;=8\\
K_g \in R^{100\times64} , 1&amp;lt;=g&amp;lt;=2\\
V_g \in R^{100\times64} , 1&amp;lt;=g&amp;lt;=2\\\]

&lt;p&gt;这里的$K_g$和$V_g$对于$Q_i,Q_{i+1},Q_{i+2},Q_{i+3}$是一样的&lt;/p&gt;

&lt;p&gt;最后再将注意力头拼接，映射回模型维度大小，因此$Y \in R^{100\times512}$
\(Y=GQA(X)=Concat(head_1,head_2,...,head_8)W^O\\
W^O \in R^{512\times512}\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;小结&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;从上面的计算过程分析可知：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;注意力头的计算量并没有减少，但是计算$K$和$V$的线性投影矩阵大小和计算量都发生了变化&lt;/li&gt;
  &lt;li&gt;需要存储的$K$和$V$大小发生了变化，缩小$\frac{h}{num_groups}$倍&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实践中，GQA只应用到decoder层，不应用到encoder层，因为encoder层的token输入不会在后续发生变化；decoder因为输入会逐渐增大，进而需要缓存越来越多的KV数据。&lt;/p&gt;

&lt;h2 id=&quot;参考信息&quot;&gt;参考信息&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2302.13971&quot;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2305.13245&quot;&gt;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.09864&quot;&gt;ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 01 Mar 2025 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2025/03/01/Llama/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2025/03/01/Llama/</guid>
        
        <category>Computer science</category>
        
        <category>LLM</category>
        
        <category>LLama</category>
        
        
      </item>
    
      <item>
        <title>The transformer's decoding</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;decoding&quot; src=&quot;/img/transformers/MIT-GrandDecoder-01-press.jpg&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;decoding strategy&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;decoding&quot; src=&quot;/img/transformers/decoding-strategies-for-transformers-thumbnail.webp&quot; width=&quot;400&quot; height=&quot;200&quot; alt=&quot;decoding strategy&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;在&lt;em&gt;The transformer’s details&lt;/em&gt;一文中，Decoder最后一层$Softmax$输出预测token在词汇表是各个token的概率大小。&lt;/p&gt;

&lt;p&gt;实际使用中，预测的token是连续生成的，那么怎么挑选预测的token会对结果有不同的影响&lt;/p&gt;

&lt;h2 id=&quot;解码策略&quot;&gt;解码策略&lt;/h2&gt;

&lt;h3 id=&quot;greedy-search&quot;&gt;greedy search&lt;/h3&gt;

&lt;p&gt;顾名思义，预测的每个token都是top-1概率大小的token。优势是简单效率高，不足之处在于步步最优不一定代表整体最优，很有可能陷入了局部最优。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;greed&quot; src=&quot;https://miro.medium.com/v2/resize:fit:4800/format:webp/1*peGpWtPOF1FrvUt0FrEJ9Q.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;greedy search&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;如上图，按照贪婪搜索的方法，最终的预测序列是&lt;strong&gt;The cat is&lt;/strong&gt;，但是从整体结果来看&lt;strong&gt;the red fox&lt;/strong&gt;才是最佳答案。可以看出，贪婪搜索会忽略当前低概率token后续的高概率token。&lt;/p&gt;

&lt;h3 id=&quot;sampling&quot;&gt;sampling&lt;/h3&gt;

&lt;h4 id=&quot;random-sampling&quot;&gt;random sampling&lt;/h4&gt;

&lt;p&gt;从模型输出的token概率分布中，随机选择一个预测token&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;random sampling&quot; src=&quot;/img/transformers/problem_with_random_sampling.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;decoding strategy&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;参考上图，使用随机采样的方法，&lt;em&gt;I set my cat down on the&lt;/em&gt;后面很有可能跟&lt;strong&gt;web&lt;/strong&gt;或&lt;strong&gt;monkey&lt;/strong&gt;，因为词汇表中大部分的token概率数值都较低，整个词汇表概率分布呈现长尾现象。&lt;/p&gt;

&lt;h4 id=&quot;random-sampling-with-temperatue&quot;&gt;random sampling with temperatue&lt;/h4&gt;

\[P(Y_t=i)=\frac{exp(\frac{z_i}{T})}{\sum_jexp(\frac{z_j}{T})}\]

&lt;p&gt;引入$T$来计算输出token的概率分布，$T$大小与词汇表token概率分布的关系如下图&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;random sampling&quot; src=&quot;/img/transformers/decoding/random_sampling_temperature.png&quot; width=&quot;500&quot; height=&quot;250&quot; alt=&quot;decoding strategy&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;可以看出，温度越高，预测token整体概率分布越趋于均衡，左侧分布幅度越小。这也侧面说明，温度越高，文本预测结果越适合创作场景，因为不需要对结果有很高的准确度约束。&lt;/p&gt;

&lt;h4 id=&quot;top-k-sampling&quot;&gt;top-k sampling&lt;/h4&gt;

&lt;p&gt;只考虑概率最大的k个token，通常$k \in [10,50]$&lt;/p&gt;

&lt;p&gt;这样仍存在选取不太可能的token作为输出的情况，比如概率分布左侧尖锐尾部很长的场景&lt;/p&gt;

&lt;h4 id=&quot;nucleus--sampling&quot;&gt;nucleus  sampling&lt;/h4&gt;

&lt;p&gt;相较于&lt;em&gt;top-k&lt;/em&gt;采样，此方法不固定选择的token数目。它设定一个概率$p$阈值，选择的$k_t$个token概率和要不高于$p$&lt;/p&gt;

&lt;h4 id=&quot;小结&quot;&gt;小结&lt;/h4&gt;

&lt;p&gt;使用温度，可以改变模型的输出概率分布，使其变得更加尖锐或者平滑；top-k或者top-p可以使得输出结果更加流畅&lt;/p&gt;

&lt;p&gt;这三种方法相当于缩小与采样范围，从新的范围中选择一个token作为预测结果&lt;/p&gt;

&lt;h3 id=&quot;beam-search&quot;&gt;beam search&lt;/h3&gt;

&lt;p&gt;目标是最大概率的预测序列&lt;/p&gt;

&lt;p&gt;示意图如下，beam size为2&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;beam search&quot; src=&quot;/img/transformers/decoding/beam_search.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;decoding strategy&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;从上图可以看到，在每一轮预测，都会保留top-2结果；图中的灰色path（也称beams）就是被选择的结果。如果beam size变大，带来的计算量会显著增大。&lt;/p&gt;

&lt;p&gt;beam search适合对最终整体结果正确性要求较高的场景，不适合创作性要求高的场景&lt;/p&gt;

&lt;h2 id=&quot;小结-1&quot;&gt;小结&lt;/h2&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;trade off&quot; src=&quot;/img/transformers/decoding/decoding_strategy_tradeoff.png&quot; width=&quot;600&quot; height=&quot;300&quot; alt=&quot;decoding strategy&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;左右两端对应结果两极，通常根据场景设定对应的decoding策略参数&lt;/p&gt;

&lt;h2 id=&quot;参考信息&quot;&gt;参考信息&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.scaler.com/topics/nlp/decoding-strategies-for-transformers/&quot;&gt;Decoding Strategies for Transformers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://storage.googleapis.com/cmu-llms/2024/2024-08-26-course-intro-and-lm-basics.pdf&quot;&gt;cmu llm course&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 15 Feb 2025 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2025/02/15/transformer-decoding-strategy/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2025/02/15/transformer-decoding-strategy/</guid>
        
        <category>Computer science</category>
        
        <category>LLM</category>
        
        <category>Transformers</category>
        
        
      </item>
    
      <item>
        <title>Paged Attention</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;vLLM&quot; src=&quot;/img/vllm/vLLM_system_overview.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;vLLM system&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;

&lt;h3 id=&quot;自回归transformer模型&quot;&gt;自回归transformer模型&lt;/h3&gt;

&lt;p&gt;首先来看自回归transformer模型结构，如下图：&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;vLLM&quot; src=&quot;/img/vllm/autoregressive_decoder.png&quot; width=&quot;400&quot; height=&quot;500&quot; alt=&quot;vLLM system&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;它主要包含以下部分：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input&lt;/li&gt;
  &lt;li&gt;embedding&lt;/li&gt;
  &lt;li&gt;positional encoding&lt;/li&gt;
  &lt;li&gt;Masked MultiHead Attention&lt;/li&gt;
  &lt;li&gt;FeedForward&lt;/li&gt;
  &lt;li&gt;Linear&amp;amp;Softmax&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中&lt;strong&gt;Masked-MultiHead Attention&lt;/strong&gt;和&lt;strong&gt;FeedFoward&lt;/strong&gt;作为一个基础模块可以不断堆叠。&lt;/p&gt;

&lt;p&gt;数学表达式如下：&lt;/p&gt;

\[MaskMultiHead(Q,K,V)=Concat(head_1,head_2,\ldots,head_h)W^O \\\]

\[head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)\]

\[W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\]

\[W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\]

\[W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}\]

\[W^O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}}\]

&lt;p&gt;从$W^O$的维度可以看出，$Concat$是在$head_i$行方向上进行&lt;/p&gt;

&lt;p&gt;这里的&lt;strong&gt;Attention&lt;/strong&gt;有些特别，公示如下：&lt;/p&gt;

\[Attention(Q,K,V)=Softmax(\frac{QK^T}{\sqrt{d_k}}+M)V\]

\[M_{i,j}=0,j&amp;lt;=i\\
M_{i,j}=-\infty, i&amp;lt;j\]

&lt;p&gt;从网路结构图以及数学表达式可以看出，decoder会做大量的K,V,Q计算&lt;/p&gt;

&lt;p&gt;Q由于每次计算只跟当前输入有关，无需缓存；但是K和V会涉及到历史token，所以需要缓存。&lt;/p&gt;

&lt;h3 id=&quot;kv-cache-matters&quot;&gt;kv cache matters&lt;/h3&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;vLLM&quot; src=&quot;/img/vllm/kv_cache_现状.png&quot; width=&quot;800&quot; height=&quot;200&quot; alt=&quot;current kv cache&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;针对KV进行缓存，一个直观的实现方法是在GPU内存上开辟连续空间用于存储，空间一部分用来存储历史token的KV矩阵，另一部分用来存储预测token的KV，当前prompt结果生成后再进行空间销毁。&lt;/p&gt;

&lt;p&gt;这有些类似我们在程序内部不断进行new和delete操作，从而带来内存碎片。同样地，直接缓存会导致大量的内外部碎片（如上图所示），降低内存使用效率，折损LLM模型服务的性能。&lt;/p&gt;

&lt;h2 id=&quot;paged-attention机制解析&quot;&gt;Paged Attention机制解析&lt;/h2&gt;

&lt;p&gt;Linux上采用分页机制来缓解内存碎片同时复用内存空间，如下图所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.bottomupcs.com/chapter05/figures/linux-layout.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;虚拟地址空间的page一一映射到物理空间的page&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.bottomupcs.com/chapter05/figures/threelevel.svg&quot; alt=&quot;https://www.bottomupcs.com/ch06s09.html&quot; /&gt;&lt;/p&gt;

&lt;p&gt;根据虚拟地址空间的地址，可以找到对应物理空间page内部的地址，如上如物理页内的offset&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;vLLM&quot; src=&quot;/img/vllm/block_table_translation.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;block table&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;vLLM中&lt;strong&gt;Block&lt;/strong&gt;概念跟Linux中的&lt;strong&gt;Page&lt;/strong&gt;类似，&lt;strong&gt;Block Table&lt;/strong&gt;进行逻辑block和物理block之间的映射&lt;/p&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;vLLM&quot; src=&quot;/img/vllm/kv_cache_two_requests.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;kv_cache two requests&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Request A和B类似Linux上的Process&lt;/p&gt;

&lt;p&gt;在虚拟block层面，A和B既能拥有各自的KV存放空间相互隔离；在物理block层面，A和B进行内存复用&lt;/p&gt;

&lt;h2 id=&quot;问题与讨论&quot;&gt;问题与讨论&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;逻辑KV block和Block table大小&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于单级分页，32位Linux，需要的页表大小为4M；64位Linux，需要的页面大小为512G，基本不可行&lt;/p&gt;

&lt;p&gt;vLLm中单个block大小一般为256KB或512KB，逻辑KV block数量根据GPU内存来设置，也可以由系统自主决定；Block Table的大小跟逻辑block数量以及每个Block的映射信息大小决定&lt;/p&gt;

&lt;p&gt;由于当前逻辑block数量不会很大，不像linux process的地址空间那么大，所以暂时没有必要进行多级映射&lt;/p&gt;

&lt;h2 id=&quot;参考信息&quot;&gt;参考信息&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;Efficient Memory Management for Large Language Model Serving with PagedAttention&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 12 Feb 2025 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2025/02/12/paged-attention-one/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2025/02/12/paged-attention-one/</guid>
        
        <category>Computer science</category>
        
        <category>LLM</category>
        
        <category>Transformers</category>
        
        <category>vLLM</category>
        
        
      </item>
    
      <item>
        <title>DeepSeek V2</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/deepseek/DeepSeek-Logo.jpg&quot; width=&quot;800&quot; height=&quot;250&quot; alt=&quot;Transformer Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;本文主要介绍DeepSeek V2的模型结构并用数学表达式分析模型计算过程。&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/deepseek/deepseek_v2_transformer_block.png&quot; width=&quot;800&quot; height=&quot;400&quot; alt=&quot;Transformer Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;V2共计有60层，$h_t=5120$，注意力头数$n_h=128$，每个头的维度$R\in^{128}$&lt;/p&gt;

&lt;p&gt;除了第一层的FFN，V2将每个transformer block中的FFN都替换为MoE层。&lt;/p&gt;

&lt;p&gt;每个toekn，只会激活6个专家；V2共计参数量236B，对于每个token，激活的参数量为21B。&lt;/p&gt;

&lt;h2 id=&quot;transformer-block&quot;&gt;Transformer Block&lt;/h2&gt;

&lt;h3 id=&quot;mla&quot;&gt;MLA&lt;/h3&gt;

&lt;h4 id=&quot;kv联合压缩&quot;&gt;KV联合压缩&lt;/h4&gt;

&lt;p&gt;输入$h_t \in R^{N \times 5120}$，这里的$N$&lt;/p&gt;

\[c_t^{KV}=h_tW^{DKV}\\
W^{DKV} \in R^{5120 \times 512}\]

\[K_t^C=c_t^{KV}W^{UK} ,\quad V_t^C=c_t^{KV}W^{UV}\\
W^{UK} \in R^{512 \times (128\times128)},\quad W^{UV} \in R^{512 \times (128\times128)}\]

&lt;p&gt;输出的$K_t^C$和$V_t^C$矩阵大小为$N \times (128\times128)$&lt;/p&gt;

&lt;h4 id=&quot;q压缩&quot;&gt;Q压缩&lt;/h4&gt;

\[c_t^Q=h_tW^{DQ}\\
Q_t^C=c_t^QW^{UQ}\\
W^{DQ} \in R^{5120 \times 1536}\\
W^{UQ} \in R^{1536 \times (128\times128)}\]

&lt;p&gt;$c_t^Q \in R^{N\times1536}$&lt;/p&gt;

&lt;p&gt;输出$Q_t^C \in R^{N \times (128\times128)}$&lt;/p&gt;

&lt;h4 id=&quot;rope解耦&quot;&gt;RoPE解耦&lt;/h4&gt;

\[Q_t^{R}=RoPE(c_t^QW^{QR}),\quad K_t^{R}=RoPE(h_tW^{KR})\\
W^{QR} \in R^{1536 \times (64*128)},\quad W^{KR} \in R^{5120 \times 64}\]

&lt;p&gt;输出的$Q_t^{R}$大小为$N \times (64*128)$，$K_t^{R}$为$N\times64$&lt;/p&gt;

&lt;p&gt;此时对于每一个头，$Q_t^{C_i} \in R^{N\times128}$，$K_t^{C_i} \in R^{N\times128}$，$Q_t^{R_i} \in R^{N\times64}$&lt;/p&gt;

\[Q_{final}^i=Concat(Q_t^{C_i},Q_t^{R_i})\\
K_{final}^i=Concat(K_t^{C_i},K_t^{R})\\\]

&lt;p&gt;&lt;em&gt;Concat&lt;/em&gt;沿着行进行，输出维度&lt;/p&gt;

\[Q_{final}^i \in R^{N \times 192}\\
K_{final}^i \in R^{N \times 192}\\\]

&lt;h4 id=&quot;注意力计算&quot;&gt;注意力计算&lt;/h4&gt;

&lt;p&gt;使用128个注意力头&lt;/p&gt;

&lt;p&gt;对于每一个$head_i$，其计算如下：&lt;/p&gt;

\[head_i=softmax(\frac {Q_{final}^i{K_{final}^i}^T}{\sqrt{128+64}}){V_t^{C_i}}\\
V_t^{C_i} \in R^{N \times 128}\]

&lt;p&gt;$head_i$的输出大小为$N\times 128$&lt;/p&gt;

&lt;p&gt;最后再经过一层映射，回到$R^{5120}$维空间&lt;/p&gt;

\[O_{concat}=Concat(head_1,head_2,...,head_{64})W^O\\
W^O \in R^{(128\times128) \times 5120}\]

&lt;h3 id=&quot;moe-architecture&quot;&gt;MoE Architecture&lt;/h3&gt;

&lt;p&gt;每个MoE层包含2个共享专家层和160个路由专家层，每个专家层的输出维度是$R\in^{1536}$&lt;/p&gt;

&lt;p&gt;整体计算表达式如下：&lt;/p&gt;

\[\begin{equation*}
\mathbf{h}'_t = \mathbf{u}_t + \sum_{i=1}^{N_s} \mathrm{FFN}_i^{(s)}(\mathbf{u}_t) + \sum_{i=1}^{N_r} g_{i,t} \mathrm{FFN}_i^{(r)}(\mathbf{u}_t)
\end{equation*}\]

\[\begin{align*}
g_{i,t} &amp;amp;= \begin{cases}
s_{i,t}, &amp;amp; s_{i,t} \in \mathrm{Topk}\left( \{s_{j,t} \mid 1 \leq j \leq N_r\}, K_r \right), \\
0, &amp;amp; \text{otherwise},
\end{cases} \\
s_{i,t} &amp;amp;= \mathrm{Softmax}_i\left( \mathbf{u}_t^T \mathbf{e}_i \right)
\end{align*}\]

&lt;p&gt;其中，$N^r=160$，$N^s=2$，$u_t \in R^{N\times5120}$&lt;/p&gt;

&lt;h4 id=&quot;expert&quot;&gt;Expert&lt;/h4&gt;

&lt;p&gt;每个专家是一个独立的FFN，输入大小$N \times 5120$
，输出大小为$N \times 5120$&lt;/p&gt;

&lt;p&gt;公示如下：&lt;/p&gt;

\[Expert_i(x)=GeLU(xW_{down})W_{up}\\
W_{down} \in R^{5120 \times 1536}\\
W_{up} \in R^{1536 \times 5120}\]

&lt;p&gt;每个专家的输出大小是一样的，多个专家的输出会按照token对应专家的概率进行加权融合作为最终的专家网络输出&lt;/p&gt;

&lt;h4 id=&quot;how-to-route&quot;&gt;How to route&lt;/h4&gt;

&lt;p&gt;输入$N\times5120$，输出$N \times 160$&lt;/p&gt;

\[g_{raw}=u_tW_g\\
W_g \in R^{5120\times160}\\
s=Softmax(g_{raw})\]

&lt;p&gt;160个数值代表每个token选择对应专家的概率，按照概率大小取top-6对应的专家。&lt;/p&gt;

&lt;p&gt;被选中的专家网络，其权重将被激活；相反，没被激活的专家，其权重将不会参与计算。&lt;/p&gt;

&lt;h2 id=&quot;参考信息&quot;&gt;参考信息&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2405.04434&quot;&gt;DeepSeek V2:A Strong, Economical, and Efficient Mixture-of-Experts Language Model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 11 Feb 2025 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2025/02/11/DeepSeek-V2/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2025/02/11/DeepSeek-V2/</guid>
        
        <category>Computer science</category>
        
        <category>LLM</category>
        
        <category>DeepSeek</category>
        
        
      </item>
    
      <item>
        <title>The transformer's details</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/transformers/transformer_architecture.png&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;Transformer Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;数学表达式&quot;&gt;数学表达式&lt;/h2&gt;

&lt;h3 id=&quot;input-embedding&quot;&gt;Input embedding&lt;/h3&gt;

&lt;p&gt;假设目前输入为 $N$ 个token，经过embedding后，变为 $N \times d_f$ 矩阵，记为 $I$&lt;/p&gt;

&lt;h3 id=&quot;positional-encoding&quot;&gt;Positional encoding&lt;/h3&gt;

&lt;p&gt;对输入的各个token做位置编码，最简单的编码方式是直接将位置向量累加到对应的emdding向量中&lt;/p&gt;

&lt;p&gt;输入$N \times d_f$，输出$N \times d_f$&lt;/p&gt;

&lt;h3 id=&quot;encoding&quot;&gt;Encoding&lt;/h3&gt;

&lt;h4 id=&quot;qkv&quot;&gt;Q,K,V&lt;/h4&gt;

&lt;p&gt;假设目前输入为 $N$ 个token，经过embedding后，变为 $N \times d$ 矩阵，记为 $I$。($d=512$，下面的 $d_{model}$ 也是同样大小)
那么 $Q,K,V$ 可以通过如下计算得到：&lt;/p&gt;

\[Q = I \times W^Q\]

\[K = I \times W^K\]

\[V = I \times W^V\]

&lt;p&gt;其中 $W^Q,W^K,W^V$ 分别为 $d \times d_k, d \times d_k, d \times d_v$ 大小的矩阵，$Q,K,V$ 大小为 $N \times d_k, N \times d_k$ 和 $N \times d_v$。&lt;/p&gt;

&lt;h4 id=&quot;单头注意力&quot;&gt;单头注意力&lt;/h4&gt;

&lt;p&gt;在上一步的基础上，我们继续：&lt;/p&gt;

\[Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V\]

&lt;p&gt;$QK^T$得到$N*N$的矩阵，再按行执行$softmax$&lt;/p&gt;

&lt;p&gt;最终输出$N \times d_v$大小的矩阵&lt;/p&gt;

&lt;h4 id=&quot;多头注意力&quot;&gt;多头注意力&lt;/h4&gt;

&lt;p&gt;对于输入的$Q,K,V$&lt;/p&gt;

&lt;p&gt;我们对其特征维度按照$h$拆分，这样就变成了$h$组$Q,K,V$&lt;/p&gt;

&lt;p&gt;对于每一组进行线性变换后，执行一次Attention&lt;/p&gt;

&lt;p&gt;最后，将所有Attention结果concat起来，再进行一次线性变换&lt;/p&gt;

&lt;p&gt;公式化表达如下&lt;/p&gt;

\[MultiHead(Q,K,V)=Concat(head_1,head_2,\ldots,head_h)W^O \\
head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)\]

\[W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\]

\[W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\]

\[W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}\]

\[W^O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}}\]

&lt;p&gt;从$W^O$的维度可以看出，$Concat$是在$head_i$行方向上进行&lt;/p&gt;

&lt;p&gt;Attention论文中$h=8$，$d_{model}=512$，$d_v=d_k=\frac{d_{model}}{h}=64$&lt;/p&gt;

&lt;p&gt;最终输出大小 $N \times d_{model}$&lt;/p&gt;

&lt;h4 id=&quot;残差连接与层归一化&quot;&gt;残差连接与层归一化&lt;/h4&gt;

&lt;p&gt;公示如下：
\(y_{output}=LayerNorm(y_{MultiHead}+x)\)&lt;/p&gt;

&lt;p&gt;$y_{MultiHead}$代表多头注意力的输出，$x$代表多头注意力的输入&lt;/p&gt;

&lt;p&gt;$LayerNorm$的计算规则如下：&lt;/p&gt;

\[LayerNorm(x_i)=\frac{(x_i-\mu)}{\sigma};\mu=\frac{1}{d}\sum_{i}{x_i},\sigma=\sqrt{\frac{1}{d}\sum_{i}{x_i}}\]

&lt;p&gt;$x_i$代表一行&lt;/p&gt;

&lt;p&gt;最终输出大小 $N \times d_{model}$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;以$N=3,d_{model}=512$为例&lt;/p&gt;

&lt;p&gt;多头注意力模块的输出大小为$3 \times 512$，输入 $x \in R^{3 \times 512}$，$LayerNorm$输出大小$3 \times 512$&lt;/p&gt;

&lt;h4 id=&quot;feedforwardaddnorm&quot;&gt;FeedForward&amp;amp;Add&amp;amp;Norm&lt;/h4&gt;

&lt;p&gt;公式如下：
\(y_{output}=LayerNorm(FFN(x)+x)\)&lt;/p&gt;

&lt;p&gt;其中FFN公式如下：
\(FFN(x)=Activation(xW_0)W_1\)&lt;/p&gt;

&lt;p&gt;$w_0 \in R^{512 \times 2048}$，$w_1 \in R^{2048 \times 512}$，Activation如ReLU或GELU&lt;/p&gt;

&lt;p&gt;Add&amp;amp;Norm的操作如前所示&lt;/p&gt;

&lt;p&gt;最终输出大小$N \times d_{model}$&lt;/p&gt;

&lt;h4 id=&quot;小结&quot;&gt;小结&lt;/h4&gt;

&lt;p&gt;Encoder层是可以堆叠的，比如选择6层或者12层，每一层的输入输出维度是一样的，最终的输出作为Decoder各层的输入&lt;/p&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;

&lt;p&gt;以下模块跟Encoder机制类似，不再赘述&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;output embedding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;positional encoding和encoder&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Add&amp;amp;Norm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FFN&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;掩码多头注意力&quot;&gt;掩码多头注意力&lt;/h4&gt;

&lt;p&gt;与Encoder中的多头注意力机制类似，不同的是这里会使用掩码遮挡未来的输出，不让模型关注&lt;/p&gt;

&lt;p&gt;假设此刻Decoder的输入大小为$N_1 \times d_{model}$&lt;/p&gt;

&lt;p&gt;输出大小$N_1 \times d_{mdoel}$&lt;/p&gt;

&lt;h4 id=&quot;交叉注意力&quot;&gt;交叉注意力&lt;/h4&gt;

&lt;p&gt;公式表达如下：
\(CrossAttention(K,Q,V)=MultiHead(K,Q,V)\)&lt;/p&gt;

&lt;p&gt;其中$MultiHead$与Encoder中的表达式一致，不同的是$K$和$V$来自Encoder的输出，两者一致&lt;/p&gt;

&lt;p&gt;输出大小$N_1 \times d_{model}$&lt;/p&gt;

&lt;h4 id=&quot;linearsoftmax&quot;&gt;Linear&amp;amp;softmax&lt;/h4&gt;

\[y_{output}=Softmax(Linear(x))\]

&lt;p&gt;$Linear$负责将$x$中每一个token的特征维度，映射到词汇表大小的维度$d_{voc}$，如下&lt;/p&gt;

\[Linear(x)=xW_{voc}\]

\[W_{voc} \in R^{d_{model} \times d_{voc}}\]

&lt;p&gt;$Softmax$再对每一个toekn的线性映射结果做转换，得到$(0,1)$范围内的概率值&lt;/p&gt;

&lt;p&gt;最终输出大小$N_1 \times d_{voc}$&lt;/p&gt;

&lt;h4 id=&quot;小结-1&quot;&gt;小结&lt;/h4&gt;

&lt;p&gt;Decoder层可以堆叠，堆叠的层数跟Encoder层可以一致或不一致；对于GPT系列模型，仅需要Decoder层&lt;/p&gt;

&lt;p&gt;对于首次token预测，Decoder的输入是起始符&lt;/p&gt;

&lt;h2 id=&quot;encoder-decoder交互动态演示&quot;&gt;Encoder-Decoder交互动态演示&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://jalammar.github.io/images/t/transformer_decoding_2.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考信息&quot;&gt;参考信息&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;illustrated transformer&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 10 Feb 2025 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2025/02/10/transformer-detail/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2025/02/10/transformer-detail/</guid>
        
        <category>Computer science</category>
        
        <category>LLM</category>
        
        <category>Transformers</category>
        
        
      </item>
    
      <item>
        <title>DeepSeek R1</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;shadow&quot; src=&quot;/img/deepseek/DeepSeek-Logo.jpg&quot; width=&quot;800&quot; height=&quot;250&quot; alt=&quot;Transformer Architecture&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;本文主要介绍DeepSeek R1的模型结构并介绍其模型生产流程。&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://media.geeksforgeeks.org/wp-content/uploads/20250203194805367699/architecture.webp&quot; alt=&quot;DeepSeek R1 Architecture&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;moe-architecture&quot;&gt;MoE Architecture&lt;/h3&gt;

&lt;p&gt;R1使用256个专家，但对于每个token只会激活其中8个专家，另有一个共享专家。&lt;/p&gt;

&lt;h4 id=&quot;router-network&quot;&gt;Router Network&lt;/h4&gt;

&lt;p&gt;输入token，数量$N$；输出$N\times256$的向量&lt;/p&gt;

&lt;p&gt;第一步：输入$x \in R^{N\times7168}$投影至低维空间&lt;/p&gt;

\[h_{proj}=xW_{proj}\\
W_{proj} \in R^{7168 \times 2048}\]

&lt;p&gt;第二步：将注意力输出与投影后的输出进行concat&lt;/p&gt;

\[h_{fused}=Concat(h_{proj},h_{attn}) \in R^{N \times (2048+2048)}\]

&lt;p&gt;第三步：进行二次投影，输出大小$N \times 256$&lt;/p&gt;

\[Router=GeLU(h_{fused})W_{gate}\\
W_{gate} \in R^{4096 \times 256}\]

&lt;p&gt;256个数值代表每个token选择对应专家的概率，按照概率大小取top-8对应的专家。&lt;/p&gt;

&lt;p&gt;被选中的专家网络，其权重将被激活；相反，没被激活的专家，其权重将不会参与计算。&lt;/p&gt;

&lt;h4 id=&quot;expert&quot;&gt;Expert&lt;/h4&gt;

&lt;p&gt;每个专家是一个独立的FFN，输入大小$N \times 7168$
，输出大小为$N \times 7168$&lt;/p&gt;

&lt;p&gt;公示如下：&lt;/p&gt;

\[Expert_i(x)=GeLU(xW_{up})W_{down}\\
W_{up} \in R^{7168\times2048}\\
W_{down} \in R^{2048 \times 7168}\]

&lt;p&gt;每个专家的输出大小是一样的，多个专家的输出会按照token对应专家的概率进行加权融合作为最终的专家网络输出&lt;/p&gt;

&lt;h3 id=&quot;mla&quot;&gt;MLA&lt;/h3&gt;

&lt;h4 id=&quot;kv联合压缩&quot;&gt;KV联合压缩&lt;/h4&gt;

&lt;p&gt;输入$h_t \in R^{N \times 7168}$&lt;/p&gt;

\[c_t^{KV}=h_tW^{DKV}\\
W^{DKV} \in R^{7168 \times 128}\\
K_t=c_t^{KV}W^{UK} ,\quad V_t=c_t^{KV}W^{UV}\\
W^{UK} \in R^{128 \times 7168},\quad W^{UV} \times R^{128 \times 7168}\]

&lt;p&gt;输出的KV矩阵大小为$N \times 7168$&lt;/p&gt;

&lt;h4 id=&quot;q压缩&quot;&gt;Q压缩&lt;/h4&gt;

\[Q_t=(h_tW^{DQ})W^{UQ}\\
W^{DQ} \in R^{7168 \times 128}\\
W^{UQ} \in R^{128 \times 7168}\]

&lt;p&gt;输出$Q_t \in R^{N \times 7168}$&lt;/p&gt;

&lt;h4 id=&quot;小结&quot;&gt;小结&lt;/h4&gt;

&lt;p&gt;可以看出，相比于MHA，MLA是将K\V\Q的计算过程转换为两步骤的低秩乘法过程，从而达到减少计算量的效果&lt;/p&gt;

&lt;h4 id=&quot;rope解耦&quot;&gt;RoPE解耦&lt;/h4&gt;

\[Q_t^{R}=RoPE(h_tW^{QR}),\quad K_t^{R}=RoPE(h_tW^{KR})\\
W^{QR} \in R^{7168\times26},\quad W^{KR} \in R^{7168 \times 26}\]

&lt;p&gt;输出的$Q_t^{R}$和$K_t^{R}$大小均为$N\times26$&lt;/p&gt;

\[Q_{final}=Concat(Q_t,Q_t^{R})\\
K_{final}=Concat(K_t,K_t^{R})\]

&lt;p&gt;&lt;em&gt;Concat&lt;/em&gt;沿着行进行，输出大小$N \times 7194$&lt;/p&gt;

&lt;h4 id=&quot;注意力计算&quot;&gt;注意力计算&lt;/h4&gt;

&lt;p&gt;使用64个注意力头
对于每一个$head_i$，其计算如下：&lt;/p&gt;

\[head_i=softmax(\frac {Q_{final}K_{final}^T}{\sqrt{112}})V_t\]

&lt;p&gt;其中&lt;/p&gt;

\[Q_{final} \in R^{N \times 138}\\
K_{final} \in R^{N \times 138}\\
V_t \in R^{N \times 112}\\\]

&lt;p&gt;这里的$138=112+26$，也就是说拆分头是对原来的$7168$维度进行拆分，将RoPE的26维追加到每个拆分的112维度中&lt;/p&gt;

&lt;p&gt;$head_i$的输出大小为$N\times112$&lt;/p&gt;

&lt;p&gt;最后再经过一层映射，回到$R^{7168}$维空间&lt;/p&gt;

\[O_{concat}=Concat(head_1,head_2,...,head_{64})W^O\\
W^O \in R^{7168 \times 7168}\]

&lt;h2 id=&quot;参考信息&quot;&gt;参考信息&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.geeksforgeeks.org/deepseek-r1-technical-overview-of-its-architecture-and-innovations/&quot;&gt;DeepSeek R1: Technical Overview of Its Architecture and Innovations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://storage.googleapis.com/cmu-llms/2024/2024-09-05-architecture-advancement-on-transformers.pdf&quot;&gt;Architecture Advancement on Transformers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.51cto.com/article/809184.html&quot;&gt;深度学习中的注意力机制革命：MHA、MQA、GQA至DeepSeek MLA的演变&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI1MjU4NjUzMg==&amp;amp;mid=2247484059&amp;amp;idx=1&amp;amp;sn=f3520146ada05cde19b9bf8842d042e3&amp;amp;chksm=e83c457343bc863001d3d4ed9086cd78aab97407039b7464f2a2ba9d9eef5d6a084530d89ad6#rd&quot;&gt;DeepSeek核心架构-MLA：剖析低秩联合压缩优化KV缓存、提升推理效率的技术细节&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 09 Feb 2025 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2025/02/09/DeepSeek-R1/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2025/02/09/DeepSeek-R1/</guid>
        
        <category>Computer science</category>
        
        <category>LLM</category>
        
        <category>DeepSeek</category>
        
        
      </item>
    
      <item>
        <title>Cursor实践总结</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;cursor&quot; src=&quot;/img/cursor/Cursor-AI.jpg&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;Cursor practice&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;本篇文章会持续更新，记录Cursor的实践总结。&lt;/p&gt;

&lt;h2 id=&quot;代码生成&quot;&gt;代码生成&lt;/h2&gt;

&lt;p&gt;工作中经常需要使用脚本处理各种任务，比如日志分析、可视化数据、文本批量修改等。使用Python并借助Cursor的代码生成功能，可以大大提高工作效率。下面贴一张经常使用的Python代码Prompt生成示例。&lt;/p&gt;

&lt;h3 id=&quot;prompt示例&quot;&gt;Prompt示例&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Please&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prefect&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flow&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Goal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;Correct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invalid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmake&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Invalid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;above&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;The&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;standard&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;you&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;want&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;The&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corrected&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmake&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;number&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Accept&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invalid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;above&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;standard&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;you&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;want&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accept&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arguments&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Find&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invalid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmake&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Correct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CXX_STANDARD&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CXX_FLAGS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;standard&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;you&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;want&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;you&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;may&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;need&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pattern&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matching&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Collect&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modified&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmake&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;number&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;into&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Requirements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;The&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;should&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;follow&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;files&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;path&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;file1.cmake&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;line&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;line_number&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;path&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;file2.cmake&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;line&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;line_number&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accept&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arguments&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;拿到对应的结果后，可以不着急使用，建议继续提问：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Given&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;me&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Please&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;check&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;If&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;achieves&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;goal&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;If&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;please&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explain&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;me&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Be&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sure&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reusable&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;easy&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maintain&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;具体需要多少次提问，可以根据任务的难度来进行调整。如果代码需要实现的任务比较复杂，建议自行拆解任务，分步实现。&lt;/p&gt;

&lt;h2 id=&quot;参考信息&quot;&gt;参考信息&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Tm_2RZm8JB8&quot;&gt;Cursor Composer 使用教程&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 30 Nov 2024 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2024/11/30/cursor%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2024/11/30/cursor%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/</guid>
        
        <category>Computer science</category>
        
        <category>Deep Learning</category>
        
        <category>LLM Agent</category>
        
        
      </item>
    
      <item>
        <title>知道什么时候停止</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;When to stop&quot; src=&quot;/img/growth/when-to-stop-blog.jpeg&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;when to stop&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;这是一个相当大的主题，可以在许多领域用到，今天主要在软件开发领域聊聊这个话题。&lt;/p&gt;

&lt;p&gt;Elon Musk 和 OpenAI CEO Sam Altman 都推崇这样一个做法：你先迈出一步，再逐步精进。不要妄想等到万事俱备才开始，也不要期盼所需的前置学习都要掌握才开始，那样你永远都不会开始。&lt;/p&gt;

&lt;p&gt;为什么会有这样的论调？&lt;/p&gt;

&lt;p&gt;首先我们要承认没有绝对完美的计划，条件大差不差的时候，可以着手去做就要赶紧去做了。因为在实践中，你总会碰到意料之外的事情，这时候反过头来调整计划，反倒是一件好事。其次我们要得接受没有完美的产品，放到软件开发领域，也是如此。这时候我们就得知道在什么时候停止。&lt;/p&gt;

&lt;p&gt;为什么？为什么软件存在瑕疵不去修理？为什么产品存在问题不去改进？&lt;/p&gt;

&lt;p&gt;为了回答这个问题，需要弄清楚客户想要什么，可以接受什么，不能接受什么。根据客户的需求设定边界，我们就可以知道软件要做到什么程度。对于开发人员来说，够好即可，但不是没有边界的好。这个客户当然可以是我们自己，我们给自己安排任务，何时停止，也就是做到何种程度即可，也是很重要的，事事完美是很要命的。任务不可能都一样重要，都一样紧急，在次要任务上过度雕琢反而耽误了主线任务的进展。&lt;/p&gt;
</description>
        <pubDate>Sun, 24 Nov 2024 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2024/11/24/%E7%9F%A5%E9%81%93%E5%9C%A8%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%81%9C%E6%AD%A2/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2024/11/24/%E7%9F%A5%E9%81%93%E5%9C%A8%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%81%9C%E6%AD%A2/</guid>
        
        <category>Growth</category>
        
        
      </item>
    
      <item>
        <title>你有权选择</title>
        <description>&lt;div&gt;
  &lt;p&gt;&lt;img class=&quot;Active working&quot; src=&quot;/img/growth/active_working.jpg&quot; width=&quot;500&quot; height=&quot;300&quot; alt=&quot;Active working&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;这个标题不是为了鼓励在工作中遇到不顺就跳槽。&lt;/p&gt;

&lt;p&gt;“你有权选择”是指人生是自己的，你要去主动选择适合自己的工作环境，工作方向以及任务内容。被动选择最大的坏处，是对自己职业生涯的不负责。&lt;/p&gt;

&lt;p&gt;被动选择，那么通常情况下是遇事才做出选择。如果不提出自己的诉求，想做的工作方向，那么分配给到自己的不太可能是你一直心心念想要做的工作。因为外人不太可能很了解你擅长什么，你的职业目标。&lt;/p&gt;

&lt;p&gt;所以，要放弃“等靠要”的思想，而是尽可能主动去思考，去贴合自己的职业目标，做一些有利于未来长期发展的事情，而不是分配啥任务就做啥任务。另外即使有些被分配的任务不得不做，也要去思考任务目标和要求是否合理，而不是真的盲从指令去做，也许按照自己的想法来做会更快更好呢？&lt;/p&gt;

&lt;p&gt;在现有的工作任务中，如果想后续做其他的任务内容，只要两者能有一定的关联，可以准备学习或者尽可能实操起来了。比如日常工作中包含了许多琐事，但是又想做一些AI工作任务，那么为何不可首先思考使用AI优化这些琐事，让这些琐事做起来更省时省力,这样也为日后转做AI任务积攒了经验。&lt;/p&gt;

&lt;p&gt;没有人可以绝对限制你做什么，除了你自己。因此，当你有想法时，不要放之任之，多试试，多思考，或许可以打开不一样的工作局面。&lt;/p&gt;
</description>
        <pubDate>Sat, 16 Nov 2024 00:00:00 +0800</pubDate>
        <link>https://peterlau123.github.io/2024/11/16/%E4%BD%A0%E6%9C%89%E6%9D%83%E9%80%89%E6%8B%A9/</link>
        <guid isPermaLink="true">https://peterlau123.github.io/2024/11/16/%E4%BD%A0%E6%9C%89%E6%9D%83%E9%80%89%E6%8B%A9/</guid>
        
        <category>Growth</category>
        
        
      </item>
    
  </channel>
</rss>
