---
title: "Dive into DeepSeek"
subtitle: "DeepSeek R1"
layout: post
author: "Peter Lau"
published: true
header-style: text
tags:
  - Computer science
  - LLM 
---


## 模型结构

![DeepSeek R1 architecture](https://media.geeksforgeeks.org/wp-content/uploads/20250203194805367699/architecture.webp)


### MoE架构

expert network每次只会激活一部分权重参数，从而提高GPu资源利用率。

### MLA

Latent Vector Compression 将K和V矩阵进行低秩分解，存储latent vector相比于存储整个矩阵要降低KV cache大小。

### transformer layer optimization


## R1生产流程

![](https://www.processon.com/embed/67e7d3fe561ce271ffc13996?cid=67e7d3fe561ce271ffc13999)

<figure style="text-align: center">
    <img class="prefect logo" src="/img/deepseek/DeepSeek R1.png" width="600" height="400">
    <figcaption style="font-style: italic; color: #666;">prefect logo</figcaption>
</figure>



## 参考文献

1. https://www.geeksforgeeks.org/deepseek-r1-technical-overview-of-its-architecture-and-innovations/
2. https://storage.googleapis.com/cmu-llms/2024/2024-09-05-architecture-advancement-on-transformers.pdf