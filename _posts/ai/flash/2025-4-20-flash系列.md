---
title: "flash系列"
subtitle: "优化机制汇总"
layout: post
author: "Peter Lau"
published: false
header-style: text
tags:
  - Computer science
  - LLM
  - DeepSeek 
---


## FlashAttention

### Attention中的Softmax

$$
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}
$$

$$
Attention = Softmax(\frac{QK^T}{\sqrt{d}})V
$$





## 参考信息

1. [flashdecoding](https://crfm.stanford.edu/2023/10/12/flashdecoding.html)