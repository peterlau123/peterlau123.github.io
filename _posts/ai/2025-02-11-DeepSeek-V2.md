---
title: "DeepSeek V2"
subtitle: "Internals"
layout: post
author: "Peter Lau"
published: false
header-style: text
tags:
  - Computer science
  - LLM
  - DeepSeek
---


<div>
  <img class="shadow" src="/img/deepseek/DeepSeek-Logo.jpg" width="800" height="250" alt="Transformer Architecture">
</div>


本文主要介绍DeepSeek V2的模型结构并用数学表达式分析模型计算过程。

## Model Architecture

<div>
  <img class="shadow" src="/img/deepseek/deepseek_v2_transformer_block.png" width="800" height="400" alt="Transformer Architecture">
</div>

V2共计有60层，$h_t=5120$，注意力头数$n_h=128$，每个头的维度$R\in^{128}$

除了第一层的FFN，V2将每个transformer block中的FFN都替换为MoE层。

每个toekn，只会激活6个专家；V2共计参数量236B，对于每个token，激活的参数量为21B。

## Transformer Block

### MLA

#### KV联合压缩

输入$h_t \in R^{N \times 5120}$，这里的$N$

$$
c_t^{KV}=h_tW^{DKV}\\
W^{DKV} \in R^{5120 \times 512}
$$

$$
K_t^C=c_t^{KV}W^{UK} ,\quad V_t^C=c_t^{KV}W^{UV}\\
W^{UK} \in R^{512 \times (128\times128)},\quad W^{UV} \in R^{512 \times (128\times128)}
$$

输出的$K_t^C$和$V_t^C$矩阵大小为$N \times (128\times128)$

#### Q压缩

$$
c_t^Q=h_tW^{DQ}\\
Q_t^C=c_t^QW^{UQ}\\
W^{DQ} \in R^{5120 \times 1536}\\
W^{UQ} \in R^{1536 \times (128\times128)}
$$

$c_t^Q \in R^{N\times1536}$

输出$Q_t^C \in R^{N \times (128\times128)}$

#### RoPE解耦

$$
Q_t^{R}=RoPE(c_t^QW^{QR}),\quad K_t^{R}=RoPE(h_tW^{KR})\\
W^{QR} \in R^{1536 \times (64*128)},\quad W^{KR} \in R^{5120 \times 64}
$$

输出的$Q_t^{R}$大小为$N \times (64*128)$，$K_t^{R}$为$N\times64$

此时对于每一个头，$Q_t^{C_i} \in R^{N\times128}$，$K_t^{C_i} \in R^{N\times128}$，$Q_t^{R_i} \in R^{N\times64}$

$$
Q_{final}^i=Concat(Q_t^{C_i},Q_t^{R_i})\\
K_{final}^i=Concat(K_t^{C_i},K_t^{R})\\
$$

*Concat*沿着行进行，输出维度

$$
Q_{final}^i \in R^{N \times 192}\\
K_{final}^i \in R^{N \times 192}\\
$$

#### 注意力计算

使用128个注意力头

对于每一个$head_i$，其计算如下：

$$
head_i=softmax(\frac {Q_{final}^i{K_{final}^i}^T}{\sqrt{128+64}}){V_t^{C_i}}\\
V_t^{C_i} \in R^{N \times 128}
$$


$head_i$的输出大小为$N\times 128$


最后再经过一层映射，回到$R^{5120}$维空间

$$
O_{concat}=Concat(head_1,head_2,...,head_{64})W^O\\
W^O \in R^{(128\times128) \times 5120}
$$

### Expert

每个专家是一个独立的FFN，输入大小$N \times 7168$
，输出大小为$N \times 7168$

公示如下：

$$
Expert_i(x)=GeLU(xW_{up})W_{down}\\
W_{up} \in R^{7168\times2048}\\
W_{down} \in R^{2048 \times 7168}
$$


每个专家的输出大小是一样的，多个专家的输出会按照token对应专家的概率进行加权融合作为最终的专家网络输出

### MoE Architecture

每个MoE层包含2个共享专家层和160个路由专家层，每个专家层的输出维度是$R\in^{1536}$

#### Router Network

输入token，数量$N$；输出$N\times256$的向量

第一步：输入$x \in R^{N\times7168}$投影至低维空间

$$
h_{proj}=xW_{proj}\\
W_{proj} \in R^{7168 \times 2048}
$$

第二步：将注意力输出与投影后的输出进行concat

$$
h_{fused}=Concat(h_{proj},h_{attn}) \in R^{N \times (2048+2048)}
$$

第三步：进行二次投影，输出大小$N \times 256$

$$
Router=GeLU(h_{fused})W_{gate}\\
W_{gate} \in R^{4096 \times 256}
$$


256个数值代表每个token选择对应专家的概率，按照概率大小取top-8对应的专家。

被选中的专家网络，其权重将被激活；相反，没被激活的专家，其权重将不会参与计算。



#### 小结

可以看出，相比于MHA，MLA是将K\V\Q的计算过程转换为两步骤的低秩乘法过程，从而达到减少计算量的效果




## 参考信息

1. [DeepSeek R1: Technical Overview of Its Architecture and Innovations](https://www.geeksforgeeks.org/deepseek-r1-technical-overview-of-its-architecture-and-innovations/)