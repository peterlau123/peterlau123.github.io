---
title: "DeepSeek R1"
subtitle: "Internals"
layout: post
author: "Peter Lau"
published: false
header-style: text
tags:
  - Computer science
  - LLM
  - DeepSeek
---

## Model Architecture

![DeepSeek R1 Architecture](https://media.geeksforgeeks.org/wp-content/uploads/20250203194805367699/architecture.webp)

### MoE Architecture

The expert network activates only a subset of weight parameters at a time, improving GPU resource utilization.

### MLA

Latent Vector Compression applies low-rank decomposition to the K and V matrices, significantly reducing KV cache size compared to storing the entire matrix.

### Transformer Layer Optimization

Details on optimization techniques for transformer layers.

## R1 Production Workflow

<div>
  <img class="shadow" src="/img/deepseek/DeepSeek R1.png" width="500" height="300" alt="DeepSeek R1 Production Workflow">
</div>

## References

1. [DeepSeek R1: Technical Overview of Its Architecture and Innovations](https://www.geeksforgeeks.org/deepseek-r1-technical-overview-of-its-architecture-and-innovations/)
2. [Architecture Advancement on Transformers](https://storage.googleapis.com/cmu-llms/2024/2024-09-05-architecture-advancement-on-transformers.pdf)
