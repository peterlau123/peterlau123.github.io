---
title: "The transformer's optimizations part one"
subtitle: "Paged Attention"
layout: post
author: "Peter Lau"
published: true
header-style: text
tags:
  - Computer science
  - LLM
  - Transformers
  - vLLM 
---


<div>
  <img class="vLLM" src="/img/vllm/vLLM_system_overview.png" width="500" height="300" alt="vLLM system">
</div>

## 背景





## Paged Attention机制解析



## 参考信息

