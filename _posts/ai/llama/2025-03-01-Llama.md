---
title: "Llama"
subtitle: "Internals"
layout: post
author: "Peter Lau"
published: false
header-style: text
tags:
  - Computer science
  - LLM
  - LLama
---


<div>
  <img class="shadow" src="/img/llama/llama-logo.png" width="500" height="300" alt="Llama Architecture">
</div>

本文主要介绍Llama的模型结构。

## Model Architecture


<div>
  <img class="shadow" src="/img/llama/llama_arch.png" width="500" height="400" alt="Llama Architecture">
</div>

Llama结构整体如上，它包含如下部分：

+ Embedding
+ Transformer stack
+ RMS Norm
+ Linear
+ Softmax

Transformer blcok包含如下部分：

+ RMS Norm
+ Rotary positional Encoding
+ Grouped Multi-Query Attention
+ Add
+ RMS Norm
+ Feed Forward SwiGLU

### Pre-normalization

### RoPE

### Grouped Multi-Query Attention

<div>
  <img class="shadow" src="/img/llama/Group-Query.png" width="500" height="200" alt="Llama Architecture">
</div>

以输入100个token为例，token的embedding维度为512

注意力头数$h=8$，分组数$num\_heads=2$

输入$X \in R^{100 \times 512}$，$8$个注意力头划分为$2$个组

$$
Q_i = XW_i^Q, W_i^Q \in R^{512\times64}\\
K_j = XW_j^K, W_j^K \in R^{512\times256}\\
V_j = XW_j^V, W_j^V \in R^{512\times256}
$$

其中，$i \in [1,8]$，$j \in [1,2]$

注意力计算公式如下

$$
head_g=softmax(\frac{Q_gK_g^T}{\sqrt{d_k}})V_g\\
Q_g \in R^{100\times256}\\
K_g \in R^{100\times256}\\
V_g \in R^{100\times256}
$$

实际每个$Attention$在计算的时候，会concat起来，也就是$Q_i,Q_{i+1},Q_{i+2},Q_{i+3}$

concat起来的大小$100\times256$作为一个组，再去跟$K_g$做点乘

$$
GQA(X)=Concat(head_1,head_2,...,head_{num\_groups})W^O\\
W^O \in R^{512\times512}
$$

### SwiGLU

## 参考信息

1. [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)
2. [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/pdf/2305.13245)
