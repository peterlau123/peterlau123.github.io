---
title: "Llama"
subtitle: "Internals"
layout: post
author: "Peter Lau"
published: false
header-style: text
tags:
  - Computer science
  - LLM
  - LLama
---


<div>
  <img class="shadow" src="/img/llama/llama-logo.png" width="500" height="300" alt="Llama Architecture">
</div>

本文主要介绍Llama的模型结构。

## Model Architecture


<div>
  <img class="shadow" src="/img/llama/llama_arch.png" width="500" height="400" alt="Llama Architecture">
</div>

Llama结构整体如上，它包含如下部分：

+ Embedding
+ Transformer stack
+ RMS Norm
+ Linear
+ Softmax

Transformer blcok包含如下部分：

+ RMS Norm
+ Rotary positional Encoding
+ Grouped Multi-Query Attention
+ Add
+ RMS Norm
+ Feed Forward SwiGLU

### Pre-normalization

### RoPE

### Grouped Multi-Query Attention

<div>
  <img class="shadow" src="/img/llama/Group-Query.png" width="500" height="200" alt="Llama Architecture">
</div>

以输入100个token为例，token的embedding维度为512

注意力头数$h=8$，分组数$num\_heads=2$

输入$X \in R^{100 \times 512}$，$8$个注意力头划分为$2$个组

$$
Q = XW^Q, W^Q \in R^{512\times512}\\
K = XW^K, W^K \in R^{512\times128}\\
V = XW^V, W^V \in R^{512\times128}
$$

此时的$Q \in R^{100 \times 512}$，$K \in R^{100 \times 128}$，$V \in R^{100 \times 128}$

每个注意力头的维度为64，每组k和v的维度为64且被组内4个注意力头共享

注意力计算公式如下

$$
head_i=softmax(\frac{Q_iK_g^T}{\sqrt{d_k}})V_g\\
Q_i \in R^{100\times64} , 1<=i<=8\\
K_g \in R^{100\times64} , 1<=g<=2\\
V_g \in R^{100\times64} , 1<=g<=2\\
$$

这里的$K_g$和$V_g$对于$Q_i,Q_{i+1},Q_{i+2},Q_{i+3}$是一样的

最后再将注意力头拼接，映射回模型维度大小，因此$Y \in R^{100\times512}$
$$
Y=GQA(X)=Concat(head_1,head_2,...,head_8)W^O\\
W^O \in R^{512\times512}
$$

**小结**

从上面的计算过程分析可知：

+ 注意力头的计算量并没有减少，但是计算$K$和$V$的线性投影矩阵大小和计算量都发生了变化
+ 对于每个token，需要存储的$K$和$V$大小发生了变化，缩小$\frac{h}{num\_groups}$倍

实践中，GQA只应用到decoder层，不应用到encoder层，因为encoder层的token输入不会在后续发生变化；decoder因为输入会逐渐增大，进而需要缓存越来越多的KV数据。

### SwiGLU

## 参考信息

1. [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)
2. [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/pdf/2305.13245)
