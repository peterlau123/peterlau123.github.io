---
title: "Llama"
subtitle: "Internals"
layout: post
author: "Peter Lau"
published: false
header-style: text
tags:
  - Computer science
  - LLM
  - LLama
---


<div>
  <img class="shadow" src="/img/llama/llama-logo.png" width="500" height="300" alt="Llama Architecture">
</div>

本文主要介绍Llama的模型结构。

## Model Architecture


<div>
  <img class="shadow" src="/img/llama/llama_arch.png" width="500" height="400" alt="Llama Architecture">
</div>

Llama结构整体如上，它包含如下部分：

+ Embedding
+ Transformer stack
+ RMS Norm
+ Linear
+ Softmax

Transformer blcok包含如下部分：

+ RMS Norm
+ Rotary positional Encoding
+ Grouped Multi-Query Attention
+ Add
+ RMS Norm
+ Feed Forward SwiGLU

### Pre-normalization

### RoPE

### Grouped Multi-Query Attention

输入$X \in R^{n \times d}$，$h$个注意力头划分为$g$个组

$$
Q_i = XW_i^Q\\
K_j = XW_j^K\\
V_j = XW_j^V
$$

其中，$i \in [1,h]$，$j \in [1,g]$

注意力计算公式如下

$$
Attention(Q_i,K_j,V_j)=softmax(\frac{Q_iK_j^T}{\sqrt{d_k}})V_j
$$

$$
GQA(X)=Concat(head_1,head_2,...,head_h)W^O
$$


### SwiGLU

## 参考信息

1. [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)
