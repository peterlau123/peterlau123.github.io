---
title: "Llama"
subtitle: "Internals"
layout: post
author: "Peter Lau"
published: false
header-style: text
tags:
  - Computer science
  - LLM
  - LLama
---


<div>
  <img class="shadow" src="/img/llama/llama-logo.png" width="500" height="300" alt="Llama Architecture">
</div>

本文主要介绍Llama的模型结构。

## Model Architecture


<div>
  <img class="shadow" src="/img/llama/llama_arch.png" width="500" height="400" alt="Llama Architecture">
</div>

Llama结构整体如上，它包含如下部分：

+ Embedding
+ Transformer stack
+ RMS Norm
+ Linear
+ Softmax

Transformer blcok包含如下部分：

+ RMS Norm
+ Rotary positional Encoding
+ Grouped Multi-Query Attention
+ Add
+ RMS Norm
+ Feed Forward SwiGLU

### Pre-normalization

### RoPE

### Grouped Multi-Query Attention

以输入100个token为例，token的embedding维度为2048

输入$X \in R^{100 \times 2048}$，$32$个注意力头划分为$4$个组

$$
Q_i = XW_i^Q, W_i^Q \in R^{2048\times64}\\
K_j = XW_j^K, W_j^K \in R^{2048\times512}\\
V_j = XW_j^V, W_j^V \in R^{2048\times512}
$$

其中，$i \in [1,32]$，$j \in [1,4]$

注意力计算公式如下

$$
Attention(Q_i,K_j,V_j)=softmax(\frac{Q_iK_j^T}{\sqrt{d_k}})V_j
$$

实际每个$Attention$在计算的时候，会concat起来，也就是$Q_i,Q_{i+1},Q_{i+2},Q_{i+3}$

concat起来的大小$100\times512$，再去跟$K_j$做点乘

$$
GQA(X)=Concat(head_1,head_2,...,head_h)W^O
$$

### SwiGLU

## 参考信息

1. [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)
